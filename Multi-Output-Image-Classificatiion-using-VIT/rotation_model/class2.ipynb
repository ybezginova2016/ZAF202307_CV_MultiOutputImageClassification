{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GySyVU4wOX2V"
      },
      "outputs": [],
      "source": [
        "! rm -rf *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMywoZ6caGud"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WzgM3wHONDM"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle\n",
        "! mkdir -p ~/.kaggle\n",
        "! mv kaggle.json ~/.kaggle/ \n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG_xhxowOOWj"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d kwentar/blur-dataset\n",
        "!unzip -qq blur-dataset.zip -d ./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5xx9xLvR2SB"
      },
      "source": [
        "<h2> Before moving to images folder, Augment ! </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SK8SM_LR1DA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "sharp_folder = \"sharp\"\n",
        "rotated_folder = \"rotated_sharp\"\n",
        "\n",
        "# Create rotated subdirectories\n",
        "os.makedirs(os.path.join(rotated_folder, \"No_rotation\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(rotated_folder, \"90_degrees_clockwise\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(rotated_folder, \"90_degrees_counterclockwise\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(rotated_folder, \"180_degrees\"), exist_ok=True)\n",
        "\n",
        "# Rotate images and save to subdirectories\n",
        "for img_filename in os.listdir(sharp_folder):\n",
        "    img = cv2.imread(os.path.join(sharp_folder, img_filename))\n",
        "    if img is None:\n",
        "        continue  # skip if unable to read image\n",
        "    \n",
        "    img_no_rot = img.copy()\n",
        "    img_90cw = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
        "    img_90ccw = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "    img_180 = cv2.rotate(img, cv2.ROTATE_180)\n",
        "\n",
        "    # Save images to appropriate subdirectory\n",
        "    cv2.imwrite(os.path.join(rotated_folder, \"No_rotation\", img_filename), img_no_rot)\n",
        "    cv2.imwrite(os.path.join(rotated_folder, \"90_degrees_clockwise\", img_filename), img_90cw)\n",
        "    cv2.imwrite(os.path.join(rotated_folder, \"90_degrees_counterclockwise\", img_filename), img_90ccw)\n",
        "    cv2.imwrite(os.path.join(rotated_folder, \"180_degrees\", img_filename), img_180)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL552BdFVVTh"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "os.mkdir(\"images\")\n",
        "src = '/content/rotated_sharp'\n",
        "dst = '/content/images'\n",
        "\n",
        "for sub_dir in os.listdir(src):\n",
        "    sub_dir_path = os.path.join(src, sub_dir)\n",
        "    if os.path.isdir(sub_dir_path):\n",
        "        shutil.move(sub_dir_path, dst)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVyU3ScYZ3Hc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "! pip install transformers pytorch-lightning --quiet\n",
        "! sudo apt -qq install git-lfs\n",
        "! git config --global credential.helper store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihFempth1zK0"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "from getpass import getpass\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from requests.exceptions import HTTPError\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader\n",
        "from torchmetrics import Accuracy\n",
        "from torchvision.datasets import ImageFolder\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_r5KO0aMIsn"
      },
      "source": [
        "## Init Dataset and Split into Training and Validation Sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAV2f4hkP3xe"
      },
      "outputs": [],
      "source": [
        "data_dir = Path('images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4vCcep26Em6"
      },
      "outputs": [],
      "source": [
        "ds = ImageFolder(data_dir)\n",
        "indices = torch.randperm(len(ds)).tolist()\n",
        "n_val = math.floor(len(indices) * .15)\n",
        "train_ds = torch.utils.data.Subset(ds, indices[:-n_val])\n",
        "val_ds = torch.utils.data.Subset(ds, indices[-n_val:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbzsRQRe6iY5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "num_examples_per_class = 5\n",
        "i = 1\n",
        "for class_idx, class_name in enumerate(ds.classes):\n",
        "    folder = ds.root / class_name\n",
        "    for image_idx, image_path in enumerate(sorted(folder.glob('*'))):\n",
        "        if image_path.suffix in ds.extensions:\n",
        "            image = Image.open(image_path)\n",
        "            plt.subplot(len(ds.classes), num_examples_per_class, i)\n",
        "            ax = plt.gca()\n",
        "            ax.set_title(\n",
        "                class_name,\n",
        "                size='xx-large',\n",
        "                pad=5,\n",
        "                loc='left',\n",
        "                y=0,\n",
        "                backgroundcolor='white'\n",
        "            )\n",
        "            ax.axis('off')\n",
        "            plt.imshow(image)\n",
        "            i += 1\n",
        "\n",
        "            if image_idx + 1 == num_examples_per_class:\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96LqcgYfLGc8"
      },
      "source": [
        "## Preparing Labels for Our Model's Config\n",
        "\n",
        "By adding `label2id` + `id2label` to our model's config, we'll get friendlier labels in the inference API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh0yxRKDGDd5"
      },
      "outputs": [],
      "source": [
        "label2id = {}\n",
        "id2label = {}\n",
        "\n",
        "for i, class_name in enumerate(ds.classes):\n",
        "    label2id[class_name] = str(i)\n",
        "    id2label[str(i)] = class_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1aly6Vrx2mM"
      },
      "source": [
        "## Image Classification Collator\n",
        "\n",
        "To apply our transforms to images, we'll use a custom collator class. We'll initialize it using an instance of `ViTFeatureExtractor` and pass the collator instance to `torch.utils.data.DataLoader`'s `collate_fn` kwarg."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2n0Mf1u1ymf"
      },
      "outputs": [],
      "source": [
        "class ImageClassificationCollator:\n",
        "    def __init__(self, feature_extractor):\n",
        "        self.feature_extractor = feature_extractor\n",
        " \n",
        "    def __call__(self, batch):\n",
        "        encodings = self.feature_extractor([x[0] for x in batch], return_tensors='pt')\n",
        "        encodings['labels'] = torch.tensor([x[1] for x in batch], dtype=torch.long)\n",
        "        return encodings "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T62uFtK7LTcz"
      },
      "source": [
        "## Init Feature Extractor, Model, Data Loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YovQELKD4Bu8"
      },
      "outputs": [],
      "source": [
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    'google/vit-base-patch16-224-in21k',\n",
        "    num_labels=len(label2id),\n",
        "    label2id=label2id,\n",
        "    id2label=id2label\n",
        ")\n",
        "collator = ImageClassificationCollator(feature_extractor)\n",
        "train_loader = DataLoader(train_ds, batch_size=8, collate_fn=collator, num_workers=2, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=8, collate_fn=collator, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEgf32rC7pQh"
      },
      "source": [
        "# Training\n",
        "\n",
        "âš¡ We'll use [PyTorch Lightning](https://pytorchlightning.ai/) to fine-tune our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIIRpEzW4LFo"
      },
      "outputs": [],
      "source": [
        "class Classifier(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model, lr: float = 2e-5, **kwargs):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters('lr', *list(kwargs))\n",
        "        self.model = model\n",
        "        self.forward = self.model.forward\n",
        "        self.val_acc = Accuracy(\n",
        "            task='multiclass' if model.config.num_labels > 2 else 'binary',\n",
        "            num_classes=model.config.num_labels\n",
        "        )\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        outputs = self(**batch)\n",
        "        self.log(f\"train_loss\", outputs.loss)\n",
        "        return outputs.loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        outputs = self(**batch)\n",
        "        self.log(f\"val_loss\", outputs.loss)\n",
        "        acc = self.val_acc(outputs.logits.argmax(1), batch['labels'])\n",
        "        self.log(f\"val_acc\", acc, prog_bar=True)\n",
        "        return outputs.loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTWYA1rf3Heg"
      },
      "outputs": [],
      "source": [
        "pl.seed_everything(42)\n",
        "classifier = Classifier(model, lr=2e-5)\n",
        "trainer = pl.Trainer(accelerator='gpu', devices=1, precision=32, max_epochs=12)\n",
        "trainer.fit(classifier, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDfUUwH73LSq"
      },
      "outputs": [],
      "source": [
        "val_batch = next(iter(val_loader))\n",
        "outputs = model(**val_batch)\n",
        "print('Preds: ', outputs.logits.softmax(1).argmax(1))\n",
        "print('Labels:', val_batch['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oLH8Ccp1t0T"
      },
      "outputs": [],
      "source": [
        "# save the model\n",
        "model_dir = \"./\"\n",
        "model_name = \"vit_model\"\n",
        "model.save_pretrained(model_dir + model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6KRfXMv1zqP"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Load the saved model\n",
        "model_dir = \"./\"\n",
        "model_name = \"vit_model\"\n",
        "loaded_model = ViTForImageClassification.from_pretrained(model_dir + model_name)\n",
        "\n",
        "# Load and preprocess the image\n",
        "image_path = \"/content/90anticlock.jpg\"\n",
        "image = Image.open(image_path)\n",
        "inputs = feature_extractor(image, return_tensors=\"pt\")\n",
        "\n",
        "# Make the prediction\n",
        "outputs = loaded_model(**inputs)\n",
        "predicted_class = id2label[str(outputs.logits.argmax(1).item())]\n",
        "\n",
        "print(f\"Predicted class: {predicted_class}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIGhFk6TDUhW"
      },
      "outputs": [],
      "source": [
        " !zip -r model.zip /content/vit_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZO-VIgCEuDI"
      },
      "outputs": [],
      "source": [
        "! du -sh /content/vit_model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
