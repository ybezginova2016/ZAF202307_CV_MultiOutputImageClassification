{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-21T10:03:16.328485Z","iopub.execute_input":"2023-02-21T10:03:16.329043Z","iopub.status.idle":"2023-02-21T10:03:16.336373Z","shell.execute_reply.started":"2023-02-21T10:03:16.328999Z","shell.execute_reply":"2023-02-21T10:03:16.333704Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"!pip install -q patchify","metadata":{"execution":{"iopub.status.busy":"2023-02-21T10:03:16.338583Z","iopub.execute_input":"2023-02-21T10:03:16.344153Z","iopub.status.idle":"2023-02-21T10:03:31.741138Z","shell.execute_reply.started":"2023-02-21T10:03:16.344115Z","shell.execute_reply":"2023-02-21T10:03:31.738535Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!rm -rf /kaggle/working/dset\n!mkdir /kaggle/working/dset\n!mkdir /kaggle/working/model\n!cp -r /kaggle/input/blur-dataset/defocused_blurred /kaggle/input/blur-dataset/motion_blurred /kaggle/input/blur-dataset/sharp /kaggle/working/dset/\n","metadata":{"execution":{"iopub.status.busy":"2023-02-21T10:03:31.748416Z","iopub.execute_input":"2023-02-21T10:03:31.749587Z","iopub.status.idle":"2023-02-21T10:03:52.962388Z","shell.execute_reply.started":"2023-02-21T10:03:31.749536Z","shell.execute_reply":"2023-02-21T10:03:52.961136Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nmkdir: cannot create directory ‘/kaggle/working/model’: File exists\n/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n","output_type":"stream"}]},{"cell_type":"code","source":"base_dir = \"/kaggle/working/dset\"\nos.listdir(base_dir)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-21T10:03:52.967160Z","iopub.execute_input":"2023-02-21T10:03:52.968115Z","iopub.status.idle":"2023-02-21T10:03:52.976388Z","shell.execute_reply.started":"2023-02-21T10:03:52.968070Z","shell.execute_reply":"2023-02-21T10:03:52.975234Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"['motion_blurred', 'defocused_blurred', 'sharp']"},"metadata":{}}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-02-21T10:03:52.977897Z","iopub.execute_input":"2023-02-21T10:03:52.978908Z","iopub.status.idle":"2023-02-21T10:03:54.279319Z","shell.execute_reply.started":"2023-02-21T10:03:52.978873Z","shell.execute_reply":"2023-02-21T10:03:54.278166Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nTue Feb 21 10:03:54 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   72C    P0    32W /  70W |  14454MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   72C    P0    32W /  70W |  14454MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\n\nclass ClassToken(Layer):\n    def __init__(self):\n        super().__init__()\n\n    def build(self, input_shape):\n        w_init = tf.random_normal_initializer()\n        self.w = tf.Variable(\n            initial_value = w_init(shape=(1, 1, input_shape[-1]), dtype=tf.float32),\n            trainable = True\n        )\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        hidden_dim = self.w.shape[-1]\n\n        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])\n        cls = tf.cast(cls, dtype=inputs.dtype)\n        return cls\n\ndef mlp(x, cf):\n    x = Dense(cf[\"mlp_dim\"], activation=\"gelu\")(x)\n    x = Dropout(cf[\"dropout_rate\"])(x)\n    x = Dense(cf[\"hidden_dim\"])(x)\n    x = Dropout(cf[\"dropout_rate\"])(x)\n    return x\n\ndef transformer_encoder(x, cf):\n    skip_1 = x\n    x = LayerNormalization()(x)\n    x = MultiHeadAttention(\n        num_heads=cf[\"num_heads\"], key_dim=cf[\"hidden_dim\"]\n    )(x, x)\n    x = Add()([x, skip_1])\n\n    skip_2 = x\n    x = LayerNormalization()(x)\n    x = mlp(x, cf)\n    x = Add()([x, skip_2])\n\n    return x\n\ndef ViT(cf):\n    '''Input layer'''\n    input_shape = (cf[\"num_patches\"], cf[\"patch_size\"]*cf[\"patch_size\"]*cf[\"num_channels\"])\n    inputs = Input(shape=input_shape)\n    # print(inputs.shape)\n\n    \"\"\" Patch + Position Embeddings \"\"\"\n    patch_embed = Dense(cf[\"hidden_dim\"])(inputs)   ## (None, 256, 768)\n    # print(patch_embed.shape)\n\n    positions = tf.range(start=0, limit=cf[\"num_patches\"], delta=1)\n    pos_embed = Embedding(input_dim=cf[\"num_patches\"], output_dim=cf[\"hidden_dim\"])(positions) ## (256, 768)\n    embed = patch_embed + pos_embed ## (None, 256, 768)\n\n    \"\"\" Adding Class Token \"\"\"\n    token = ClassToken()(embed)\n    x = Concatenate(axis=1)([token, embed]) ## (None, 257, 768)\n\n    for _ in range(cf[\"num_layers\"]):\n        x = transformer_encoder(x, cf)\n\n    \"\"\" Classification Head \"\"\"\n    x = LayerNormalization()(x)     ## (None, 257, 768)\n    x = x[:, 0, :]\n    x = Dense(cf[\"num_classes\"], activation=\"softmax\")(x)\n\n    model = Model(inputs, x)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-02-21T10:03:54.284383Z","iopub.execute_input":"2023-02-21T10:03:54.286626Z","iopub.status.idle":"2023-02-21T10:03:54.312298Z","shell.execute_reply.started":"2023-02-21T10:03:54.286583Z","shell.execute_reply":"2023-02-21T10:03:54.311376Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport cv2\nfrom glob import glob\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom patchify import patchify\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2023-02-21T10:03:54.317536Z","iopub.execute_input":"2023-02-21T10:03:54.320106Z","iopub.status.idle":"2023-02-21T10:03:54.327972Z","shell.execute_reply.started":"2023-02-21T10:03:54.320071Z","shell.execute_reply":"2023-02-21T10:03:54.327007Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"\"\"\" Hyperparameters \"\"\"\nhp = {}\nhp[\"image_size\"] = 200\nhp[\"num_channels\"] = 3\nhp[\"patch_size\"] = 25\nhp[\"num_patches\"] = (hp[\"image_size\"]**2) // (hp[\"patch_size\"]**2)\nhp[\"flat_patches_shape\"] = (hp[\"num_patches\"], hp[\"patch_size\"]*hp[\"patch_size\"]*hp[\"num_channels\"])\n\nhp[\"batch_size\"] = 16\nhp[\"lr\"] = 1e-4\nhp[\"num_epochs\"] = 500\nhp[\"num_classes\"] = 3\nhp[\"class_names\"] = ['F', 'M', 'S'] # F = defocused_blurred, M = Motion_blurred, S = Sharp\n\nhp[\"num_layers\"] = 12\nhp[\"hidden_dim\"] = 768\nhp[\"mlp_dim\"] = 3072\nhp[\"num_heads\"] = 12\nhp[\"dropout_rate\"] = 0.1","metadata":{"execution":{"iopub.status.busy":"2023-02-21T10:03:54.332670Z","iopub.execute_input":"2023-02-21T10:03:54.335385Z","iopub.status.idle":"2023-02-21T10:03:54.345357Z","shell.execute_reply.started":"2023-02-21T10:03:54.335352Z","shell.execute_reply":"2023-02-21T10:03:54.344413Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def create_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\ndef load_data(path, split=0.1):\n\n    #images extension is jpg and JPG\n    valid_exts = [\".jpg\", \".jpeg\"]\n\n    # Recursively search for image files in all subdirectories under root_dir\n    images = []\n    for dirpath, dirnames, filenames in os.walk(path):\n        for filename in filenames:\n            # Check if the file has a valid image file extension\n            if any(filename.lower().endswith(ext) for ext in valid_exts):\n                # Append the file path to the list of image files\n                images.append(os.path.join(dirpath, filename))\n\n    np.random.shuffle(images)\n\n    split_size = int(split*len(images))\n\n    # print(split_size)\n    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)\n\n    return train_x, valid_x, test_x\n\ndef process_image_label(path):\n    \"\"\" Reading images \"\"\"\n    path = path.decode()\n    image = cv2.imread(path, cv2.IMREAD_COLOR)\n    image = cv2.resize(image, (hp[\"image_size\"], hp[\"image_size\"]))\n    image = image / 255.0\n\n    \"\"\" Preprocessing to patches \"\"\"\n    patch_shape = (hp[\"patch_size\"], hp[\"patch_size\"], hp[\"num_channels\"])\n    patches = patchify(image, patch_shape, hp[\"patch_size\"])\n\n    # Flatten the patches\n    patches = patches.reshape(hp[\"flat_patches_shape\"])\n    # print(patches.shape)\n    patches = patches.astype(np.float32)\n\n\n    # Save the first 64 patches as images for debugging purposes\n    # for i in range(64):\n    #     cv2.imwrite(\"files/{}.jpg\".format(i), patches[i])\n\n\n    \"\"\" Label \"\"\"\n    class_name = path.split('_')[-1].split('.')[0]\n    # print(class_name)\n    class_index = hp[\"class_names\"].index(class_name)\n    class_index = np.array(class_index, dtype=np.int32)\n    return patches, class_index\n    \n#to use the function in tensorlow we have to parse the functions\ndef parse(path):\n    patches, labels = tf.numpy_function(process_image_label, [path], [tf.float32, tf.int32])\n    labels = tf.one_hot(labels, hp[\"num_classes\"])\n\n    patches.set_shape([hp[\"num_patches\"], hp[\"patch_size\"]*hp[\"patch_size\"]*hp[\"num_channels\"]])\n    labels.set_shape([hp[\"num_classes\"]])\n\n    return patches, labels\n\ndef tf_dataset(images, batch=32):\n    dataset = tf.data.Dataset.from_tensor_slices(images)\n    dataset = dataset.map(parse).batch(batch).prefetch(8)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-02-21T10:03:54.360571Z","iopub.execute_input":"2023-02-21T10:03:54.363516Z","iopub.status.idle":"2023-02-21T10:03:54.626910Z","shell.execute_reply.started":"2023-02-21T10:03:54.363475Z","shell.execute_reply":"2023-02-21T10:03:54.625601Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n\n    #seed the environment\n    np.random.seed(42)\n    tf.random.set_seed(42)\n\n    create_dir(\"files\")\n\n    #paths to the dataset, model and history\n    dset_path = '/kaggle/working/dset'\n    model_path = os.path.join('/kaggle/working/model', 'model.h5')\n    history_path = os.path.join('/kaggle/working/model', 'log.csv')\n\n    #load the data\n    train_x, valid_x, test_x = load_data(dset_path)\n    print(f\"Train: {len(train_x)} Valid: {len(valid_x)} Test: {len(test_x)}\")\n\n    # create the dataset\n    train_dataset = tf_dataset(train_x, batch=hp[\"batch_size\"])\n    valid_dataset = tf_dataset(valid_x, batch=hp[\"batch_size\"])\n\n    #create the model\n    \"\"\" Model \"\"\"\n    model = ViT(hp)\n    model.compile(\n        loss=\"categorical_crossentropy\",\n        optimizer=tf.keras.optimizers.Adam(hp[\"lr\"], clipvalue=1.0),\n        metrics=[\"acc\"]\n    )\n\n    # as the kaggle provides two gpus, we can use both of them\n    strategy = tf.distribute.MirroredStrategy()\n\n    with strategy.scope():\n        model = ViT(hp)\n        model.compile(\n            loss=\"categorical_crossentropy\",\n            optimizer=tf.keras.optimizers.Adam(hp[\"lr\"], clipvalue=1.0),\n            metrics=[\"acc\"]\n        )\n\n        callbacks = [\n            ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True),\n            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-10, verbose=1),\n            CSVLogger(history_path, append=True),\n            EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=False),\n        ]\n\n    # train the model\n    history = model.fit(\n        train_dataset,\n        validation_data=valid_dataset,\n        epochs=hp[\"num_epochs\"],\n        callbacks=callbacks\n    )","metadata":{"execution":{"iopub.status.busy":"2023-02-21T10:03:54.633371Z","iopub.execute_input":"2023-02-21T10:03:54.636585Z","iopub.status.idle":"2023-02-21T11:04:04.651076Z","shell.execute_reply.started":"2023-02-21T10:03:54.636532Z","shell.execute_reply":"2023-02-21T11:04:04.650116Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Train: 840 Valid: 105 Test: 105\nEpoch 1/500\n53/53 [==============================] - ETA: 0s - loss: 1.8034 - acc: 0.3393\nEpoch 1: val_loss improved from inf to 1.18161, saving model to /kaggle/working/model/model.h5\n53/53 [==============================] - 115s 2s/step - loss: 1.8034 - acc: 0.3393 - val_loss: 1.1816 - val_acc: 0.3238 - lr: 1.0000e-04\nEpoch 2/500\n53/53 [==============================] - ETA: 0s - loss: 1.2561 - acc: 0.3262\nEpoch 2: val_loss improved from 1.18161 to 1.12775, saving model to /kaggle/working/model/model.h5\n53/53 [==============================] - 86s 2s/step - loss: 1.2561 - acc: 0.3262 - val_loss: 1.1278 - val_acc: 0.3238 - lr: 1.0000e-04\nEpoch 3/500\n53/53 [==============================] - ETA: 0s - loss: 1.2020 - acc: 0.3214\nEpoch 3: val_loss improved from 1.12775 to 1.12391, saving model to /kaggle/working/model/model.h5\n53/53 [==============================] - 82s 2s/step - loss: 1.2020 - acc: 0.3214 - val_loss: 1.1239 - val_acc: 0.3238 - lr: 1.0000e-04\nEpoch 4/500\n53/53 [==============================] - ETA: 0s - loss: 1.1976 - acc: 0.3060\nEpoch 4: val_loss improved from 1.12391 to 1.11137, saving model to /kaggle/working/model/model.h5\n53/53 [==============================] - 86s 2s/step - loss: 1.1976 - acc: 0.3060 - val_loss: 1.1114 - val_acc: 0.3238 - lr: 1.0000e-04\nEpoch 5/500\n53/53 [==============================] - ETA: 0s - loss: 1.1907 - acc: 0.3107\nEpoch 5: val_loss improved from 1.11137 to 1.09873, saving model to /kaggle/working/model/model.h5\n53/53 [==============================] - 87s 2s/step - loss: 1.1907 - acc: 0.3107 - val_loss: 1.0987 - val_acc: 0.3524 - lr: 1.0000e-04\nEpoch 6/500\n53/53 [==============================] - ETA: 0s - loss: 1.1760 - acc: 0.3226\nEpoch 6: val_loss improved from 1.09873 to 1.09300, saving model to /kaggle/working/model/model.h5\n53/53 [==============================] - 82s 2s/step - loss: 1.1760 - acc: 0.3226 - val_loss: 1.0930 - val_acc: 0.3810 - lr: 1.0000e-04\nEpoch 7/500\n53/53 [==============================] - ETA: 0s - loss: 1.1718 - acc: 0.3060\nEpoch 7: val_loss did not improve from 1.09300\n53/53 [==============================] - 62s 1s/step - loss: 1.1718 - acc: 0.3060 - val_loss: 1.0945 - val_acc: 0.3810 - lr: 1.0000e-04\nEpoch 8/500\n53/53 [==============================] - ETA: 0s - loss: 1.1601 - acc: 0.3238\nEpoch 8: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 1.1601 - acc: 0.3238 - val_loss: 1.0979 - val_acc: 0.3905 - lr: 1.0000e-04\nEpoch 9/500\n53/53 [==============================] - ETA: 0s - loss: 1.1593 - acc: 0.3179\nEpoch 9: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 1.1593 - acc: 0.3179 - val_loss: 1.0992 - val_acc: 0.3905 - lr: 1.0000e-04\nEpoch 10/500\n53/53 [==============================] - ETA: 0s - loss: 1.1567 - acc: 0.3131\nEpoch 10: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 1.1567 - acc: 0.3131 - val_loss: 1.1120 - val_acc: 0.3905 - lr: 1.0000e-04\nEpoch 11/500\n53/53 [==============================] - ETA: 0s - loss: 1.1523 - acc: 0.3238\nEpoch 11: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 1.1523 - acc: 0.3238 - val_loss: 1.1170 - val_acc: 0.3905 - lr: 1.0000e-04\nEpoch 12/500\n53/53 [==============================] - ETA: 0s - loss: 1.1472 - acc: 0.3119\nEpoch 12: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 1.1472 - acc: 0.3119 - val_loss: 1.1168 - val_acc: 0.3905 - lr: 1.0000e-04\nEpoch 13/500\n53/53 [==============================] - ETA: 0s - loss: 1.1418 - acc: 0.3226\nEpoch 13: val_loss did not improve from 1.09300\n53/53 [==============================] - 59s 1s/step - loss: 1.1418 - acc: 0.3226 - val_loss: 1.1131 - val_acc: 0.3905 - lr: 1.0000e-04\nEpoch 14/500\n53/53 [==============================] - ETA: 0s - loss: 1.1393 - acc: 0.3202\nEpoch 14: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 1.1393 - acc: 0.3202 - val_loss: 1.1137 - val_acc: 0.3905 - lr: 1.0000e-04\nEpoch 15/500\n53/53 [==============================] - ETA: 0s - loss: 1.1318 - acc: 0.3310\nEpoch 15: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 1.1318 - acc: 0.3310 - val_loss: 1.1156 - val_acc: 0.4000 - lr: 1.0000e-04\nEpoch 16/500\n53/53 [==============================] - ETA: 0s - loss: 1.1289 - acc: 0.3393\nEpoch 16: val_loss did not improve from 1.09300\n\nEpoch 16: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n53/53 [==============================] - 60s 1s/step - loss: 1.1289 - acc: 0.3393 - val_loss: 1.1158 - val_acc: 0.3810 - lr: 1.0000e-04\nEpoch 17/500\n53/53 [==============================] - ETA: 0s - loss: 1.0711 - acc: 0.4179\nEpoch 17: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 1.0711 - acc: 0.4179 - val_loss: 1.1251 - val_acc: 0.3048 - lr: 1.0000e-05\nEpoch 18/500\n53/53 [==============================] - ETA: 0s - loss: 1.0683 - acc: 0.4119\nEpoch 18: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 1.0683 - acc: 0.4119 - val_loss: 1.1347 - val_acc: 0.3048 - lr: 1.0000e-05\nEpoch 19/500\n53/53 [==============================] - ETA: 0s - loss: 1.0615 - acc: 0.4190\nEpoch 19: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 1.0615 - acc: 0.4190 - val_loss: 1.1426 - val_acc: 0.3143 - lr: 1.0000e-05\nEpoch 20/500\n53/53 [==============================] - ETA: 0s - loss: 1.0532 - acc: 0.4369\nEpoch 20: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 1.0532 - acc: 0.4369 - val_loss: 1.1496 - val_acc: 0.3429 - lr: 1.0000e-05\nEpoch 21/500\n53/53 [==============================] - ETA: 0s - loss: 1.0447 - acc: 0.4560\nEpoch 21: val_loss did not improve from 1.09300\n53/53 [==============================] - 59s 1s/step - loss: 1.0447 - acc: 0.4560 - val_loss: 1.1603 - val_acc: 0.3333 - lr: 1.0000e-05\nEpoch 22/500\n53/53 [==============================] - ETA: 0s - loss: 1.0351 - acc: 0.4595\nEpoch 22: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 1.0351 - acc: 0.4595 - val_loss: 1.1721 - val_acc: 0.3238 - lr: 1.0000e-05\nEpoch 23/500\n53/53 [==============================] - ETA: 0s - loss: 1.0229 - acc: 0.4643\nEpoch 23: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 1.0229 - acc: 0.4643 - val_loss: 1.1776 - val_acc: 0.3619 - lr: 1.0000e-05\nEpoch 24/500\n53/53 [==============================] - ETA: 0s - loss: 1.0023 - acc: 0.4893\nEpoch 24: val_loss did not improve from 1.09300\n53/53 [==============================] - 59s 1s/step - loss: 1.0023 - acc: 0.4893 - val_loss: 1.1618 - val_acc: 0.3524 - lr: 1.0000e-05\nEpoch 25/500\n53/53 [==============================] - ETA: 0s - loss: 0.9734 - acc: 0.5250\nEpoch 25: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.9734 - acc: 0.5250 - val_loss: 1.2196 - val_acc: 0.3524 - lr: 1.0000e-05\nEpoch 26/500\n53/53 [==============================] - ETA: 0s - loss: 0.9404 - acc: 0.5298\nEpoch 26: val_loss did not improve from 1.09300\n\nEpoch 26: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n53/53 [==============================] - 59s 1s/step - loss: 0.9404 - acc: 0.5298 - val_loss: 1.2555 - val_acc: 0.3429 - lr: 1.0000e-05\nEpoch 27/500\n53/53 [==============================] - ETA: 0s - loss: 0.9023 - acc: 0.5762\nEpoch 27: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.9023 - acc: 0.5762 - val_loss: 1.2555 - val_acc: 0.4000 - lr: 1.0000e-06\nEpoch 28/500\n53/53 [==============================] - ETA: 0s - loss: 0.8817 - acc: 0.5988\nEpoch 28: val_loss did not improve from 1.09300\n53/53 [==============================] - 59s 1s/step - loss: 0.8817 - acc: 0.5988 - val_loss: 1.2592 - val_acc: 0.4000 - lr: 1.0000e-06\nEpoch 29/500\n53/53 [==============================] - ETA: 0s - loss: 0.8727 - acc: 0.6000\nEpoch 29: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8727 - acc: 0.6000 - val_loss: 1.2661 - val_acc: 0.3905 - lr: 1.0000e-06\nEpoch 30/500\n53/53 [==============================] - ETA: 0s - loss: 0.8657 - acc: 0.5988\nEpoch 30: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8657 - acc: 0.5988 - val_loss: 1.2736 - val_acc: 0.4190 - lr: 1.0000e-06\nEpoch 31/500\n53/53 [==============================] - ETA: 0s - loss: 0.8593 - acc: 0.6155\nEpoch 31: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8593 - acc: 0.6155 - val_loss: 1.2866 - val_acc: 0.4000 - lr: 1.0000e-06\nEpoch 32/500\n53/53 [==============================] - ETA: 0s - loss: 0.8509 - acc: 0.6107\nEpoch 32: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8509 - acc: 0.6107 - val_loss: 1.2868 - val_acc: 0.3714 - lr: 1.0000e-06\nEpoch 33/500\n53/53 [==============================] - ETA: 0s - loss: 0.8484 - acc: 0.6036\nEpoch 33: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8484 - acc: 0.6036 - val_loss: 1.3006 - val_acc: 0.3905 - lr: 1.0000e-06\nEpoch 34/500\n53/53 [==============================] - ETA: 0s - loss: 0.8413 - acc: 0.6131\nEpoch 34: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8413 - acc: 0.6131 - val_loss: 1.3126 - val_acc: 0.4095 - lr: 1.0000e-06\nEpoch 35/500\n53/53 [==============================] - ETA: 0s - loss: 0.8351 - acc: 0.6214\nEpoch 35: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8351 - acc: 0.6214 - val_loss: 1.3261 - val_acc: 0.4190 - lr: 1.0000e-06\nEpoch 36/500\n53/53 [==============================] - ETA: 0s - loss: 0.8295 - acc: 0.6155\nEpoch 36: val_loss did not improve from 1.09300\n\nEpoch 36: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n53/53 [==============================] - 59s 1s/step - loss: 0.8295 - acc: 0.6155 - val_loss: 1.3374 - val_acc: 0.4095 - lr: 1.0000e-06\nEpoch 37/500\n53/53 [==============================] - ETA: 0s - loss: 0.8212 - acc: 0.6274\nEpoch 37: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8212 - acc: 0.6274 - val_loss: 1.3383 - val_acc: 0.4095 - lr: 1.0000e-07\nEpoch 38/500\n53/53 [==============================] - ETA: 0s - loss: 0.8214 - acc: 0.6286\nEpoch 38: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8214 - acc: 0.6286 - val_loss: 1.3394 - val_acc: 0.4095 - lr: 1.0000e-07\nEpoch 39/500\n53/53 [==============================] - ETA: 0s - loss: 0.8202 - acc: 0.6286\nEpoch 39: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8202 - acc: 0.6286 - val_loss: 1.3402 - val_acc: 0.4095 - lr: 1.0000e-07\nEpoch 40/500\n53/53 [==============================] - ETA: 0s - loss: 0.8170 - acc: 0.6345\nEpoch 40: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8170 - acc: 0.6345 - val_loss: 1.3421 - val_acc: 0.4095 - lr: 1.0000e-07\nEpoch 41/500\n53/53 [==============================] - ETA: 0s - loss: 0.8185 - acc: 0.6393\nEpoch 41: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8185 - acc: 0.6393 - val_loss: 1.3439 - val_acc: 0.4095 - lr: 1.0000e-07\nEpoch 42/500\n53/53 [==============================] - ETA: 0s - loss: 0.8179 - acc: 0.6310\nEpoch 42: val_loss did not improve from 1.09300\n53/53 [==============================] - 59s 1s/step - loss: 0.8179 - acc: 0.6310 - val_loss: 1.3456 - val_acc: 0.4095 - lr: 1.0000e-07\nEpoch 43/500\n53/53 [==============================] - ETA: 0s - loss: 0.8134 - acc: 0.6345\nEpoch 43: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8134 - acc: 0.6345 - val_loss: 1.3474 - val_acc: 0.4095 - lr: 1.0000e-07\nEpoch 44/500\n53/53 [==============================] - ETA: 0s - loss: 0.8137 - acc: 0.6333\nEpoch 44: val_loss did not improve from 1.09300\n53/53 [==============================] - 59s 1s/step - loss: 0.8137 - acc: 0.6333 - val_loss: 1.3491 - val_acc: 0.4095 - lr: 1.0000e-07\nEpoch 45/500\n53/53 [==============================] - ETA: 0s - loss: 0.8159 - acc: 0.6298\nEpoch 45: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8159 - acc: 0.6298 - val_loss: 1.3503 - val_acc: 0.4095 - lr: 1.0000e-07\nEpoch 46/500\n53/53 [==============================] - ETA: 0s - loss: 0.8157 - acc: 0.6345\nEpoch 46: val_loss did not improve from 1.09300\n\nEpoch 46: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n53/53 [==============================] - 60s 1s/step - loss: 0.8157 - acc: 0.6345 - val_loss: 1.3515 - val_acc: 0.4095 - lr: 1.0000e-07\nEpoch 47/500\n53/53 [==============================] - ETA: 0s - loss: 0.8132 - acc: 0.6274\nEpoch 47: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8132 - acc: 0.6274 - val_loss: 1.3517 - val_acc: 0.4095 - lr: 1.0000e-08\nEpoch 48/500\n53/53 [==============================] - ETA: 0s - loss: 0.8126 - acc: 0.6238\nEpoch 48: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8126 - acc: 0.6238 - val_loss: 1.3518 - val_acc: 0.4095 - lr: 1.0000e-08\nEpoch 49/500\n53/53 [==============================] - ETA: 0s - loss: 0.8142 - acc: 0.6321\nEpoch 49: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8142 - acc: 0.6321 - val_loss: 1.3519 - val_acc: 0.4095 - lr: 1.0000e-08\nEpoch 50/500\n53/53 [==============================] - ETA: 0s - loss: 0.8134 - acc: 0.6345\nEpoch 50: val_loss did not improve from 1.09300\n53/53 [==============================] - 65s 1s/step - loss: 0.8134 - acc: 0.6345 - val_loss: 1.3520 - val_acc: 0.4095 - lr: 1.0000e-08\nEpoch 51/500\n53/53 [==============================] - ETA: 0s - loss: 0.8142 - acc: 0.6250\nEpoch 51: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8142 - acc: 0.6250 - val_loss: 1.3521 - val_acc: 0.4095 - lr: 1.0000e-08\nEpoch 52/500\n53/53 [==============================] - ETA: 0s - loss: 0.8102 - acc: 0.6333\nEpoch 52: val_loss did not improve from 1.09300\n53/53 [==============================] - 59s 1s/step - loss: 0.8102 - acc: 0.6333 - val_loss: 1.3522 - val_acc: 0.4095 - lr: 1.0000e-08\nEpoch 53/500\n53/53 [==============================] - ETA: 0s - loss: 0.8166 - acc: 0.6310\nEpoch 53: val_loss did not improve from 1.09300\n53/53 [==============================] - 59s 1s/step - loss: 0.8166 - acc: 0.6310 - val_loss: 1.3523 - val_acc: 0.4095 - lr: 1.0000e-08\nEpoch 54/500\n53/53 [==============================] - ETA: 0s - loss: 0.8160 - acc: 0.6345\nEpoch 54: val_loss did not improve from 1.09300\n53/53 [==============================] - 60s 1s/step - loss: 0.8160 - acc: 0.6345 - val_loss: 1.3524 - val_acc: 0.4095 - lr: 1.0000e-08\nEpoch 55/500\n53/53 [==============================] - ETA: 0s - loss: 0.8133 - acc: 0.6345\nEpoch 55: val_loss did not improve from 1.09300\n53/53 [==============================] - 59s 1s/step - loss: 0.8133 - acc: 0.6345 - val_loss: 1.3525 - val_acc: 0.4095 - lr: 1.0000e-08\nEpoch 56/500\n53/53 [==============================] - ETA: 0s - loss: 0.8140 - acc: 0.6345\nEpoch 56: val_loss did not improve from 1.09300\n\nEpoch 56: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n53/53 [==============================] - 60s 1s/step - loss: 0.8140 - acc: 0.6345 - val_loss: 1.3527 - val_acc: 0.4095 - lr: 1.0000e-08\n","output_type":"stream"}]},{"cell_type":"code","source":"for device in tf.config.list_physical_devices():\n    print(\": {}\".format(device.name))","metadata":{"execution":{"iopub.status.busy":"2023-02-21T11:04:04.652798Z","iopub.execute_input":"2023-02-21T11:04:04.653195Z","iopub.status.idle":"2023-02-21T11:04:04.658827Z","shell.execute_reply.started":"2023-02-21T11:04:04.653158Z","shell.execute_reply":"2023-02-21T11:04:04.657642Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":": /physical_device:CPU:0\n: /physical_device:GPU:0\n: /physical_device:GPU:1\n","output_type":"stream"}]}]}