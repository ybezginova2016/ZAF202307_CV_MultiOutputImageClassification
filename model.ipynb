{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqIDQWIBAaFQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bl8FGUYAaFS"
   },
   "source": [
    "## 1 - Load the data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "93f64bgjAaFT"
   },
   "outputs": [],
   "source": [
    "dataset_dir = 'C:/Users/himan/ML/PycharmProjects/DeepLearning/Yulia_classification/data/'\n",
    "IMG_SIZE = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "\n",
    "categories = ['defocused_blurred', 'motion_blurred','sharp']\n",
    "training_bgr = []\n",
    "training_rgb = []\n",
    "\n",
    "# saving data with respect to RGB and BGR\n",
    "def create_training_data():\n",
    "    for i in range(0,2,1):\n",
    "        print(i)\n",
    "        for category in categories:\n",
    "            path = dataset_dir+category\n",
    "            print(path)\n",
    "            class_num = categories.index(category)\n",
    "\n",
    "            for img in os.listdir(path):\n",
    "                if i == 0:\n",
    "                    img_array = cv2.imread(os.path.join(path, img))\n",
    "                    img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                    training_bgr.append([img_array, class_num])\n",
    "                if i==1:\n",
    "                    img_array = cv2.imread(os.path.join(path, img))  \n",
    "                    img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
    "                    img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                    training_rgb.append([img_array, class_num])\n",
    "                    \n",
    "        i+=1\n",
    "                    \n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "def preprocessing(training, n):\n",
    "    # shuffling the dataset\n",
    "    random.shuffle(training)\n",
    "\n",
    "    # assigning labels and features\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    for features, label in training:\n",
    "        X.append(features)\n",
    "        y.append(label)\n",
    "\n",
    "    # resizing features in accordance with CNN\n",
    "    X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "    # Normalising X and converting labels to categorical features\n",
    "    X = X.astype('float32')\n",
    "    X /= 255\n",
    "\n",
    "    y = np_utils.to_categorical(y,n)\n",
    "\n",
    "    # splitting X and y for use in CNN\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each aforementioned class has three types of data with the class labels,  thus data for each of the three classes has the following images:\n",
    "* Original Images from the link\n",
    "* Rotated Images\n",
    "* BGR and RGB Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = [0, 270, 90, 180] # (0 degree, 90 degree clockwise, 90 degree anticlockwise, 180 degree or flipped image)\n",
    "\n",
    "def rotate(image, angle):\n",
    "    image = imutils.rotate(image, angle=angle) # this rotates image in anti-clockwise direction\n",
    "    return image\n",
    "\n",
    "training_rotate = []\n",
    "\n",
    "for i in range(len(training_rgb)):\n",
    "    for angle in angles:\n",
    "        class_num = angles.index(angle)\n",
    "        training_rgb[i][0] = rotate(training_rgb[i][0], angle)\n",
    "        training_rotate.append([training_rgb[i][0], class_num]) \n",
    "\n",
    "for i in range(len(training_bgr)):\n",
    "    for angle in angles:\n",
    "        class_num = angles.index(angle)\n",
    "        training_bgr[i][0] = rotate(training_bgr[i][0], angle)\n",
    "        training_rotate.append([training_bgr[i][0], class_num])  \n",
    "\n",
    "X_train_rotate, X_test_rotate, y_train_rotate, y_test_rotate = preprocessing(training_rotate, len(angles))\n",
    "len(training_rotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_quality = []\n",
    "\n",
    "for i in range(len(training_rgb)):\n",
    "    for angle in angles:\n",
    "        training_rgb[i][0] = rotate(training_rgb[i][0], angle)\n",
    "        training_quality.append([training_rgb[i][0], training_rgb[i][1]]) \n",
    "        \n",
    "for i in range(len(training_bgr)):\n",
    "    for angle in angles:\n",
    "        training_bgr[i][0] = rotate(training_bgr[i][0], angle)\n",
    "        training_quality.append([training_bgr[i][0], training_bgr[i][1]])  \n",
    "         \n",
    "\n",
    "X_train_quality, X_test_quality, y_train_quality, y_test_quality = preprocessing(training_quality, len(categories))\n",
    "len(training_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = ['RGB', 'BGR']\n",
    "training_mode = []\n",
    "\n",
    "for i in range(len(training_rgb)):\n",
    "    for angle in angles:\n",
    "        training_rgb[i][0] = rotate(training_rgb[i][0], angle)\n",
    "        training_mode.append([training_rgb[i][0], mode.index('RGB')])\n",
    "        \n",
    "for i in range(len(training_bgr)):\n",
    "    for angle in angles:\n",
    "        training_bgr[i][0] = rotate(training_bgr[i][0], angle)\n",
    "        training_mode.append([training_bgr[i][0], mode.index('BGR')])  \n",
    "\n",
    "X_train_mode, X_test_mode, y_train_mode, y_test_mode = preprocessing(training_mode, len(mode))\n",
    "len(training_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-DPySESAaFU"
   },
   "source": [
    "## 2 - Models\n",
    "Write a code for training, validating, testing for all the models you are capable of in the notebook `model.ipynb`. Put as much comments as you can what you do, how the pre-trained model works, which issues you face. We want to deploy your model into production, please help us to do so by clearly saying what you do and why it is good.\n",
    "\n",
    "**Write a clear conclusion which model can be recommended to the client at the of this part.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5j5o_TxAaFU"
   },
   "outputs": [],
   "source": [
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='loss', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Squeeze and Excitation(SE)** block consists of two operations: \n",
    "* A squeeze operation and an excitation operation. The squeeze operation is a global pooling operation that reduces the spatial dimensions of the feature maps produced by the previous convolutional layer to a single channel. \n",
    "* The excitation operation then applies a set of fully connected layers to this single-channel representation, which produces a set of weights that are applied to the original feature maps to emphasize or suppress specific channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SE_block(inputs, ratio=8):\n",
    "    b, h, w, c = inputs.shape\n",
    "    # squeeze \n",
    "    x = GlobalAveragePooling2D()(inputs)\n",
    "    x = Reshape((1, 1, c))(x)\n",
    "    # extraction\n",
    "    x = Dense(c//ratio, activation='relu', kernel_initializer='glorot_uniform', use_bias=False)(x)\n",
    "    x = Dense(c, activation='sigmoid', kernel_initializer='glorot_uniform', use_bias=False)(x)\n",
    "\n",
    "    # scaling\n",
    "    x = Multiply()([inputs, x]) # x*inputs\n",
    "\n",
    "    return x\n",
    "\n",
    "# its returns tensor with original input shape\n",
    "\n",
    "inputs = Input(shape=(128, 128, 32))\n",
    "print(SE_block(inputs).shape)\n",
    "\n",
    "def create_model(name):\n",
    "\n",
    "    input_tensor = Input(shape=[IMG_SIZE, IMG_SIZE, 3], name=name)\n",
    "    x = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu')(input_tensor)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = SE_block(x)\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), padding='same',activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = SE_block(x)\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), padding='same',activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    # fully connected\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    output_tensor = Dense(9, activation='relu')(x)\n",
    "    model = Model(inputs=[input_tensor], outputs=[output_tensor])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# fittin the model\n",
    "# borrowed from Yulia\n",
    "\n",
    "model_quality = create_model('class1')\n",
    "model_rotate = create_model('class2')\n",
    "model_mode = create_model('class3')\n",
    "\n",
    "# segregating out ouput layers for three different classes\n",
    "mergedOutput = concatenate([model_quality.output, model_rotate.output, model_mode.output])\n",
    "out_quality = Dense(3, activation='softmax', name='quality')(mergedOutput)\n",
    "out_rotate = Dense(4, activation='softmax', name='rotate')(mergedOutput)\n",
    "out_mode = Dense(2, activation='softmax', name='mode')(mergedOutput)\n",
    "\n",
    "merged_model = Model(inputs=[model_quality.input, model_rotate.input, model_mode.input],\n",
    "                     outputs=[out_quality, out_rotate, out_mode])\n",
    "\n",
    "print(merged_model.summary())\n",
    "\n",
    "merged_model.compile(optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=\"categorical_accuracy\") \n",
    "\n",
    "history = merged_model.fit(x={'class1': X_train_quality, 'class2': X_train_rotate, 'class3': X_train_mode},\n",
    "                 y={'quality': y_train_quality, 'rotate': y_train_rotate, 'mode': y_train_mode},\n",
    "                 batch_size=64, epochs=25, callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpGVLMBqAaFV"
   },
   "source": [
    "## 3 - Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxaopCVzAaFV"
   },
   "outputs": [],
   "source": [
    "# saving the model\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = merged_model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "merged_model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Conclusion\n",
    "\n",
    "* Accuracy regarding to Mode and Rotation maxed out before 4 epochs since we are using SE blocks, which extracts information from channels.\n",
    "* For the second and third Classes we can definitely use SE blocks\n",
    "* Regarding the quality class we can definitely do better with better data quality."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
